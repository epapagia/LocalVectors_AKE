{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LV_AKE.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNMdNok60JcdOlfM5o/cecJ"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"E4XsK9T_HtVV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1594107599674,"user_tz":-180,"elapsed":995,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"e9584c78-2636-45c1-e4bb-d104067012c3"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"C3hMHKq5IwNo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1594107599675,"user_tz":-180,"elapsed":977,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"d0a28157-b4e0-44c9-b6e7-f3d08b9f4411"},"source":["%cd /content/gdrive/My\\ Drive/Colab_notebooks/Official_GitHub_Repo_LV"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Colab_notebooks/Official_GitHub_Repo_LV\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6tW4Ni_Murht","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107599676,"user_tz":-180,"elapsed":963,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["# !unzip pke.zip"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZEjD7PbDvAZX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107599677,"user_tz":-180,"elapsed":952,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["# !git clone https://github.com/boudinfl/ake-datasets.git"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9gpernExqmQ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107600148,"user_tz":-180,"elapsed":1414,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["# !git clone http://github.com/stanfordnlp/glove"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"0hOb_iCOx4cH","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107600149,"user_tz":-180,"elapsed":1404,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["# %cd glove"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"iUJ138V3yDaD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107600150,"user_tz":-180,"elapsed":1390,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["# !make"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"wWSpKMgMIzou","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":84},"executionInfo":{"status":"ok","timestamp":1594107601745,"user_tz":-180,"elapsed":2973,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"ac85cab2-0c3d-4187-dca2-0d7713846a0d"},"source":["#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","import matplotlib.pyplot as plt\n","\n","import nltk\n","nltk.download('stopwords')\n","from nltk.corpus import stopwords\n","from nltk import SnowballStemmer\n","\n","import pke\n","from pke import LoadFile\n","\n","import os\n","from os import listdir\n","\n","import string\n","\n","import sys\n","\n","import numpy as np\n","\n","import operator\n","\n","import re\n","\n","from Functions import Functions\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"x1qLKmyKI6pS","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594107601747,"user_tz":-180,"elapsed":2954,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["mode = 'test'\n","# 'train'\n","# 'test'\n","\n","pca_dim = 'NoPCA'"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"v4jwOZxQJF4k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1594107601748,"user_tz":-180,"elapsed":2939,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"d6dd69c6-7f10-4ce5-d58e-2950a8ea534d"},"source":["stopwords_extend = []\n","with open('./stopwords_snowball_expanded.txt') as f:\n","    stopwords_extend = f.read().splitlines()\n","stopwords_list = stopwords_extend\n","stopwords_list += list(string.punctuation)\n","stopwords_list += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n","stopwords_list += stopwords.words('english')\n","stopwords_list_stemmed = []\n","ps = SnowballStemmer('porter')\n","for stopw in stopwords_list:\n","    stopwords_list_stemmed.append(ps.stem(stopw))\n","stopwords_list += stopwords_list_stemmed\n","stopwords_list.extend(['\\t', '\\n', '\\x0b', '\\x0c', '\\r', ' '])\n","common_adjectives = []\n","with open('./common_adjectives.txt') as f:\n","    common_adjectives = f.read().splitlines()\n","print(common_adjectives)\n","reporting_verbs = []\n","with open('./reporting_verbs.txt') as f:\n","    reporting_verbs = f.read().splitlines()\n","print(reporting_verbs)\n","determiners = []\n","with open('./determiners.txt') as f:\n","    determiners = f.read().splitlines()\n","print(determiners)\n","functional_words = []\n","with open('./functional_words.txt') as f:\n","    functional_words = f.read().splitlines()\n","print(functional_words)\n","\n","stopwords_with_capital = [stop.title() for stop in stopwords_list]\n","stopwords_list.extend(stopwords_with_capital)\n","stopwords_list = list(set(stopwords_list))"],"execution_count":10,"outputs":[{"output_type":"stream","text":["['other', 'new', 'good', 'great', 'big', 'small', 'large', 'different', 'little', 'important', 'bad', 'real', 'best', 'only', 'able', 'major', 'better', 'possible', 'true', 'easy', 'clear', 'recent', 'certain', 'difficult', 'available', 'likely', 'current', 'wrong', 'past', 'fine', 'common', 'poor', 'significant', 'similar', 'same', 'happy', 'serious', 'ready', 'various', 'entire', 'close', 'final', 'main', 'nice', 'huge', 'popular']\n","['admit', 'agree', 'believe', 'claim', 'confess', 'decide ', 'deny', 'doubt', 'explain', 'feel', 'hope', 'insist', 'mention', 'promise', 'reply', 'say', 'suggest', 'advise', 'prefer', 'propose', 'urge', 'accuse', 'apologise', 'prohibit', 'refuse', 'threaten', 'ask', 'allow', 'beg', 'encourage', 'forbid', 'instruct', 'invite', 'persuade', 'remind', 'tell', 'warn', 'want']\n","['the', 'a', 'an', 'this', 'that', 'these', 'those', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'few', 'little', 'much', 'many', 'lot', 'most', 'some', 'any', 'enough', 'all', 'both', 'half', 'either', 'neither', 'each', 'every', 'other', 'another', 'such', 'what', 'rather', 'quite']\n","['a', 'about', 'above', 'after', 'after', 'again', 'against', 'ago', 'ahead', 'all', 'almost', 'almost', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'an', 'any', 'are', \"aren't\", 'around', 'as', 'at', 'away ', 'backward', 'backwards', 'be', 'because', 'before', 'behind', 'below', 'beneath', 'beside', 'between', 'both', 'but', 'by ', 'can', 'cannot', \"can't\", 'cause', \"'cos\", 'could', \"couldn't \", \"'d\", 'despite', 'did', \"didn't\", 'do', 'does', \"doesn't\", \"don't\", 'down', 'during ', 'each', 'either', 'even', 'ever', 'every', 'except', 'for', 'forward', 'from', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'however', 'I', 'if', 'in', 'inside', 'inspite', 'instead', 'into', 'is', \"isn't\", 'it', 'its', 'itself', 'just', \"'ll\", 'least', 'less', 'like', \"'m\", 'many', 'may', \"mayn't\", 'me', 'might', \"mightn't\", 'mine', 'more', 'most', 'much', 'must', \"mustn't\", 'my', 'myself', 'near', 'need', \"needn't\", 'needs', 'neither', 'never', 'no', 'none', 'nor', 'not', 'now', 'of', 'off', 'often', 'on', 'once', 'only', 'onto', 'or', 'ought', \"oughtn't\", 'our', 'ours', 'ourselves', 'out', 'outside', 'over ', 'past', 'perhaps ', 'quite', \"'re\", 'rather', \"'s\", 'seldom', 'several', 'shall', \"shan't\", 'she', 'should', \"shouldn't\", 'since', 'so', 'some', 'sometimes', 'soon ', 'than', 'that', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'therefore', 'these', 'they', 'this', 'those', 'though', 'through', 'thus', 'till', 'to', 'together', 'too', 'towards', 'under', 'unless', 'until', 'up', 'upon', 'us', 'used', \"usedn't\", \"usen't\", 'usually', \"'ve\", 'very', 'was', \"wasn't\", 'we', 'well', 'were', \"weren't\", 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whom', 'whose', 'why', 'will', 'with', 'without', \"won't\", 'would', \"wouldn't\", 'yet', 'you', 'your', 'yours', 'yourself', 'yourselves']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zt4qrGybJMBn","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1594108268281,"user_tz":-180,"elapsed":620,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}}},"source":["dataset_path = '/content/gdrive/My Drive/Colab_notebooks/LocalRepresentationsKE/ake-datasets/datasets/NUS/'\n","\n","dataset = dataset_path.split('/')[len(dataset_path.split('/'))-2]"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"r7IGXIdCJQIY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1594108273209,"user_tz":-180,"elapsed":793,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"4367119d-bd44-4041-cb9c-263445d4a43a"},"source":["keyphrases_folder = 'references/'\n","\n","files_subfolder = ''\n","data_files = []\n","if mode == 'train':\n","  if dataset == 'NUS':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[:111]\n","    test_author_stem = 'test.author.stem.json'\n","    test_reader_stem = 'test.reader.stem.json'\n","    test_combined_stem = 'test.combined.stem.json'\n","\n","    default_keyphrases_file = test_combined_stem\n","  elif dataset == 'SemEval-2010':\n","    files_subfolder = 'train/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    train_author_stem = 'train.author.stem.json'\n","    train_reader_stem = 'train.reader.stem.json'\n","    train_combined_stem = 'train.combined.stem.json'\n","\n","    default_keyphrases_file = train_combined_stem\n","  elif dataset == 'ACM':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[400:2304]\n","    test_author_stem = 'test.author.stem.json'\n","    \n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'PubMed':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[320:]\n","    test_author_stem = 'test.author.stem.json'\n","    \n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'Citeulike-180':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[80:]\n","    test_reader_stem = 'test.reader.stem.json'\n","    \n","    default_keyphrases_file = test_reader_stem\n","  elif dataset == 'CSTR':\n","    files_subfolder = 'train/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    train_author_stem = 'train.author.stem.json'\n","    \n","    default_keyphrases_file = train_author_stem\n","\n","elif mode == 'test':\n","  dataset = dataset_path.split('/')[len(dataset_path.split('/'))-2]\n","  if dataset == 'NUS':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[111:211]\n","    test_author_stem = 'test.author.stem.json'\n","    test_reader_stem = 'test.reader.stem.json'\n","    test_combined_stem = 'test.combined.stem.json'\n","\n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'SemEval-2010':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    test_author_stem = 'test.author.stem.json'\n","    test_reader_stem = 'test.reader.stem.json'\n","    test_combined_stem = 'test.combined.stem.json'\n","\n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'ACM':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[:400]\n","    test_author_stem = 'test.author.stem.json'\n","\n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'PubMed':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[:320]\n","    test_author_stem = 'test.author.stem.json'\n","    \n","    default_keyphrases_file = test_author_stem\n","  elif dataset == 'Citeulike-180':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    data_files = sorted(data_files)#, reverse=True)\n","    data_files = data_files[:80]\n","    test_reader_stem = 'test.reader.stem.json'\n","    \n","    default_keyphrases_file = test_reader_stem\n","  elif dataset == 'CSTR':\n","    files_subfolder = 'test/'\n","    data_files = [f for f in listdir(dataset_path+files_subfolder)]\n","    test_author_stem = 'test.author.stem.json'\n","    \n","    default_keyphrases_file = test_author_stem\n","\n","\n","print('Dataset:', dataset, 'mode:', mode, 'Number of files:', len(data_files), 'Sorted data file names:', data_files)    \n","\n","# model = fasttext.load_model(\"/content/gdrive/My Drive/Colab_notebooks/LocalRepresentationsKE/Fasttext/model_original_final.bin\")"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Dataset: NUS mode: test Number of files: 100 Sorted data file names: ['201.xml', '202.xml', '203.xml', '204.xml', '205.xml', '206.xml', '207.xml', '208.xml', '209.xml', '21.xml', '210.xml', '211.xml', '212.xml', '213.xml', '214.xml', '215.xml', '22.xml', '23.xml', '24.xml', '25.xml', '26.xml', '27.xml', '28.xml', '29.xml', '3.xml', '30.xml', '31.xml', '32.xml', '33.xml', '34.xml', '35.xml', '36.xml', '37.xml', '38.xml', '39.xml', '4.xml', '40.xml', '41.xml', '42.xml', '43.xml', '44.xml', '45.xml', '46.xml', '47.xml', '48.xml', '49.xml', '5.xml', '50.xml', '51.xml', '52.xml', '53.xml', '54.xml', '55.xml', '56.xml', '57.xml', '58.xml', '59.xml', '6.xml', '60.xml', '61.xml', '62.xml', '63.xml', '64.xml', '65.xml', '66.xml', '67.xml', '68.xml', '69.xml', '7.xml', '70.xml', '71.xml', '72.xml', '73.xml', '74.xml', '75.xml', '76.xml', '77.xml', '78.xml', '79.xml', '8.xml', '80.xml', '81.xml', '82.xml', '83.xml', '84.xml', '85.xml', '86.xml', '87.xml', '88.xml', '89.xml', '9.xml', '90.xml', '91.xml', '92.xml', '94.xml', '95.xml', '96.xml', '97.xml', '98.xml', '99.xml']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"v3UVreQTJXMp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1kt9wBPDBjdMA0g4zdKTudscDTkA21CjB"},"executionInfo":{"status":"ok","timestamp":1594108735204,"user_tz":-180,"elapsed":457777,"user":{"displayName":"Eirini Papagiannopoulou","photoUrl":"","userId":"05267738054668688141"}},"outputId":"8752a18e-4e2d-4bc1-90fe-443616625af7"},"source":["percentage_LV_list = {}\n","\n","precision_LV_list = {}\n","\n","fm_LV_list = {}\n","\n","\n","for top in range(5, 21, 5):\n","\n","    percentage_LV_list[top] = []\n","\n","    precision_LV_list[top] = []\n","\n","    fm_LV_list[top] = []\n","\n","for fvc, filevc in enumerate(data_files):\n","  functions_obj = Functions(dataset_path+files_subfolder, filevc)\n","  print(fvc, filevc)\n","  #filevc = '208.xml'\n","\n","  keyphrases = functions_obj.document_gold_keyphrases(dataset_path+keyphrases_folder+default_keyphrases_file)\n","  print(keyphrases)      \n","  if len(keyphrases) == 0:# or filevc == '29080.xml':\n","    print('No keyphrase for this file...')\n","    continue  \n","  flatten_keywords = [word for sublist in keyphrases for word in sublist]\n","  flatten_keywords = list(set(flatten_keywords))\n","  print(len(flatten_keywords), flatten_keywords)\n","\n","  extractor = LoadFile(dataset_path+files_subfolder+filevc)\n","  doc_text = extractor.read_document_from_xml_file()\n","  # print(text_doc)\n","  doc_text = doc_text.replace('\\n', ' ')\n","  doc_text = re.sub(\"\\s\\s+\", \" \", doc_text)#.translate(str.maketrans('', '', string.punctuation)))\n","  # load the content of the document, here in txt format\n","  # the input language is set to English (used for the stoplist)\n","  # normalization is set to stemming (computed with Porter's stemming algorithm)\n","  sentences = extractor.load_document(input=doc_text,\n","                                      language=\"en\",\n","                                      normalization='stemming')\n","\n","  clean_sentences, clean_bi_sentences, clean_tri_sentences, dict_realSentId_matrixSentId = extractor.get_clean_sentence(\n","      stopwords_list, common_adjectives, reporting_verbs, determiners, functional_words)\n","  \n","  # for id, sent in clean_sentences.items():\n","  #   print(id, sent)\n","  #   if id == 10:\n","  #     break\n","\n","  # extractor.ngram_selection(n=3)\n","  \n","  position_matrix, vocab, ivocab = extractor.ngram_position_matrix()\n","  print(\"position_matrix preview\", position_matrix, position_matrix.shape)\n","  # print(position_matrix.shape, len(vocab.keys()), len(clean_sentences.keys()))\n","\n","  cooccurrence_dict = extractor.build_cooccurrences(window=10)\n","  cooccurrence_matrix = extractor.get_cooccurrences_matrix()\n","  # print(\"cooccurrence_matrix preview\", cooccurrence_matrix, cooccurrence_matrix.shape)\n","  # print(cooccurrence_matrix.shape, len(vocab.keys()), len(vocab.keys()))\n","  pca_cooccurrence_matrix = np.zeros(shape=(1, 1))\n","  if pca_dim == 'NoPCA':\n","    pca_cooccurrence_matrix = cooccurrence_matrix\n","  else:\n","    pca_cooccurrence_matrix = extractor.pca_projection(cooccurrence_matrix, pca_dim)\n","\n","  \n","  # LV_topN: topN LV - original score,\n","\n","  LV_topN = extractor.tune_SC_cooccurrences(\n","      position_matrix, pca_cooccurrence_matrix, ivocab, score_type = 'custom', measure = 'euclidean')\n","  print('all LV are:', len(LV_topN), 'out of', len(ivocab.keys()))\n","  #LV_topN_list = list(dict(LV_topN).keys())\n","  print('OUTPUT OF LV:', LV_topN)\n","  LV_topN_list_all = []\n","  for u in LV_topN:\n","      LV_topN_list_all.append(u[0])\n","\n","\n","  for top in range(5, 21, 5):\n","    print('---------------------------------------------------------------------------TOP:', top, '----------------------------------------------------------------------------------------------------------------------------------------------------------')\n","\n","    LV_topN_list = LV_topN_list_all[:top]\n","\n","    percentage_LV = len(set(LV_topN_list).intersection(flatten_keywords))/float(len(flatten_keywords))\n","\n","    precision_LV = 0.0\n","    if len(LV_topN_list) > 0: \n","      precision_LV = len(set(LV_topN_list).intersection(flatten_keywords))/float(len(LV_topN_list))\n","\n","    fm_LV = 0.0\n","    if percentage_LV > 0 or precision_LV > 0:\n","      fm_LV = 2.0*(percentage_LV*precision_LV)/(percentage_LV+precision_LV)\n","\n","\n","    print(top, ': rec_LV', percentage_LV)\n","\n","    print(top, ': prec_LV', precision_LV)\n","\n","    print(top, ': fm_LV', fm_LV)\n","\n","    print('---------------------------------------------------------------------------AVERAGE----------------------------------------------------------------------------------------------------------------------------------------------------------')\n","\n","    percentage_LV_list[top].append(percentage_LV)\n","\n","    precision_LV_list[top].append(precision_LV)\n","\n","    fm_LV_list[top].append(fm_LV)\n","\n","\n","    print(top, ': rec_LV_av:', sum(percentage_LV_list[top])/len(percentage_LV_list[top])) \n","\n","    print(top, ': prec_LV_av:', sum(precision_LV_list[top])/len(precision_LV_list[top]))\n","\n","    print(top, ': fm_LV_av:', sum(fm_LV_list[top])/len(fm_LV_list[top]))\n","\n","    print('')\n","                                  "],"execution_count":16,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}]}